\documentclass[../../../include/open-logic-chapter]{subfiles}

\begin{document}

\olchapter{ads}{reinforcementlearning}{Lecture IV}

\olsection{Actor Critic methods}
\subsection{Value function}

This is the future rewards starting at s and following $\pi$

\scalebox{1.5}{%
$\textstyle V^{\pi}a( s_t ) = \sum_{t^{\primt} = t}^T \mathbb{E}_{{s}_{t^{\prime}},a_{t^{\prime}} \sim \pi} \left[r(s_{t^{\prime}},a_{t^{\prime}}) \mid s_t \right] $
}

\subsection{Q-function $\mathbb{Q}^{\pi}$}

This is the future rewards starting at s,taking a and following $\pi$

\scalebox{1.5}{%
$\textstyle V^{\pi}a( s_t ) = {\textcolor{blue}{\sum_{t^{\primt} = t}^T }}\mathbb{E}_{{s}_{t^{\prime}},a_{t^{\prime}} \sim \pi} \left[r(s_{t^{\prime}},a_{t^{\prime}}) \mid s_t \right] $
}

\subsubsection{Improving Policy Gradients}

The gradient would be better if we increased the likelihood when $\mathbb{Q}$ is high and decreased the
likelihood when $\mathbb{Q}$ is low

$\nabla_{\theta} J(\theta) \sim \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_t^i s_t^i) \mathbb{Q}(s_t^i a_t^i)$


\subsubsection{Introducing baselines}

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim P_{\theta}(\tau)} \left[\nabla_{\theta} \log P{\theta}(\tau) r(\tau) - \textcolor{blue}{\underbrace{b}_{\parbox{3cm}{subtracting  a baseline  ``constant''}}}  \right]$
}

\vskip 3ex
which is the average reward given by
\vskip 3ex
\scalebox{1.5}{%
  $b = \frac{!}{N} \sum_{i=1}^N r(\tai)$
}
\vskip 3ex
If we subtract average reward we get negatie-gradients for below-average behaviour.

The following is the proof that the subtracted value doesn't have any effect on the gradient

$ \mathbb{E}\left[\nabla_{\theta} \log \pi{\theta}(\tau) b\right] = \int p_{\theta}(\tau)\nabla_{\theta} \log \pi{\theta}(\tau) b d\tau $
Using this trick
\begin{tcolorbox}[fontupper=\Large,sharp corners, colback=gray!20, colframe=green!10!pink]

\vskip 3ex
$  P_{\theta}(\tau) \nabla_{\theta} \log  P_{\theta}(\tau) $
$ = \nabla_{\theta}P_{\theta}(\tau)$

\end{tcolorbox}

we get
\scalebox{1.5}{%
$ = b\underbrace{\nabla_{\theta} \underbrace{\int P_{\theta}(\tau) d\tau}_\text{This is 1}}_{\text{Gradient is zero}}$
}
\end{document}
