\documentclass[../../../include/open-logic-chapter]{subfiles}

\begin{document}

\olchapter{ads}{reinforcementlearning}{Lecture IV}

\olsection{Actor Critic methods}
\subsection{Value function}

This is the future rewards starting at s and following policy $\pi$

\scalebox{1.5}{%
$\textstyle V^{\pi}( s_t ) = \sum_{t^{\prime} = t}^T \mathbb{E}_{{s}_{t^{\prime}},a_{t^{\prime}} \sim \pi} \left[r(s_{t^{\prime}},a_{t^{\prime}}) \mid s_t \right] $
}

\subsection{Q-function $\mathbb{Q}^{\pi}$}

\scalebox{1.5}{%
$\textstyle Q^{\pi}( s_t,a_t ) = {\textcolor{blue}{\sum_{t^{\prime} = t}^T }}\mathbb{E}_{\pi} \left[r(s_t,a_t) \mid s_t,a_t \right] $
}

\subsubsection{Improving Policy Gradients}

The gradient would be better if we increased the likelihood when $\mathbb{Q}$ is high and decreased the
likelihood when $\mathbb{Q}$ is low

$\nabla_{\theta} J(\theta) \sim \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} s_{s,i) \mathbb{Q}(s_t^i a_t^i)$

The explanation shown below is about this.

\subsubsection{Introducing baselines}

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim P_{\theta}(\tau)} \left[\nabla_{\theta} \log P{\theta}(\tau) r(\tau) - \textcolor{blue}{\underbrace{b}_{\parbox{3cm}{subtracting  a baseline  ``constant''}}}  \right]$
}

\vskip 3ex
which is the average reward given by
\vskip 3ex
\scalebox{1.5}{%
  $b = \frac{!}{N} \sum_{i=1}^N r(\tau)$
}
\vskip 3ex
If we subtract average reward we get negatie-gradients for below-average behaviour.

The following is the proof that the subtracted value doesn't have any effect on the gradient

$ \mathbb{E}\left[\nabla_{\theta} \log \pi{\theta}(\tau) b\right] = \int p_{\theta}(\tau)\nabla_{\theta} \log \pi{\theta}(\tau) b d\tau $
Using this trick
\begin{tcolorbox}[fontupper=\Large,sharp corners, colback=gray!20, colframe=green!10!pink]

\vskip 3ex
$  P_{\theta}(\tau) \nabla_{\theta} \log  P_{\theta}(\tau) $
$ = \nabla_{\theta}P_{\theta}(\tau)$

\end{tcolorbox}

we get
\scalebox{1.5}{%
$ = b\underbrace{\nabla_{\theta} \underbrace{\int P_{\theta}(\tau) d\tau}_\text{This is 1}}_{\text{Gradient is zero}}$
}
\eject
\begin{figure}[ht]
  \includegraphics{./qfunction.tikz}
  \caption{Improving policy Gradients}
  \ollabel{fig:function}
\end{figure}

This will ameliorate the situation caused when the robot takes a small
step forward and fall backwards.

\subsubsection{Estimating future rewards using $\mathbb{Q}$ function}

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t}) \underbrace{\tcboxmath{r(s_{i,{t^{\prime}}}, a_{i,{t^{\prime}}})}}_{\parbox{3cm}{Extimate future rewards if we take {a_t} \text{ at } {s_t}}}$}



True future rewards can be obtained by using the $\mathbb{Q}$ function like this.

We will get much better results if the form of the gradient is like this.

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t}) \mathbb{Q}(a_{i,t} , s_{i,t}) $
}

\subsubsection{What about baselines ? }

Once we start using the $\mathbb{Q}$ function we get this when we subtract the baseline.

\vskip 3ex

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t}) ( \mathbb{Q}(a_{i,t} , s_{i,t}) - \underbrace{\cancel{b}}_{\mathbb{V}^\pi (s_t)} )$
}

\vskip 3ex

\begin{tcolorbox}[fontupper=\Large,sharp corners, colback=green!10, colframe=green!10!pink]
$ b = \text{ average reward } = \frac{1}{N} \sum_i \mathbb{Q}(s_{i,t}, a_{i,t})$

\vskip 3ex

$\textstyle V( s_t ) = \mathbb{E}_{a_t \sim \pi_{\theta}(\bullet \mid s_t)} \left[\mathbb{Q}(s_t,a_t) \right] $
\end{tcolorbox}

\vskip 3ex
Recall : $\mathbb{A}^{\pi}(a_t, s_t) =  \mathbb{Q}^{\pi}(a_t, s_t)-\mathbb{V}^{\pi}(s_t)$

So with baseline we get

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t}) \mathbb{A}^{\pi}(a_t, s_t)$
}

\section{Offline RL outline }

Better estimates of $\mathbb{A}$ will lead to less noisy gradients.
\vskip 3ex
${\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t})\tcboxmath[
  `colback=teal,
        colframe=green!75!black,
        boxrule=0.5pt,
        arc=2mm
] {\mathbb{A}^{\pi}(a_t, s_t)}}$

First initialize the policy ( randomly with imitation learning and heuristics )

\vskip 3ex
\section{Estimated expected return}

Should we fit $\mathbb{V}^{\pi},\mathbb{Q}^{\pi},\mathbb{A}^{\pi} \text{ ?}$. We are going to fit $\mathbb{V}^{\pi}$.


Recall : $\mathbb{A}^{\pi}(s_t, a_t) =  \mathbb{Q}^{\pi}(s_t, a_t)-\mathbb{V}^{\pi}(s_t)$. There is a way to turn this equation that only depends on $\mathbb{V}^{\pi}$. $\mathbb{Q}^{\pi}(a_t, s_t)$ starts at state s, takes action a and sums the rewards. So we can write the equation like this.
\eject
$ \underbrace{\mathbb{Q}^{\pi}(s_t, a_t)}_{\parbox{7cm}{ Rewards at the current time step and the value starting from s_{t+1}}}$


\begin{figure}[ht]
  \includegraphics{./onlinerl.tikz}
  \caption{Online RL}
  \ollabel{fig:function}
\end{figure}

So we can rewrite the equations like this.

\scalebox{1.5}{%
$\mathbb{Q}^{\pi}(s_t, a_t) = \sum_{t^{\prime} = t}^T \mathbb{E}_{\pi_{\theta}} \left[r(s_{t^{\prime}},a_{t^{\prime}}) \mid  (s_t,a_t)\right]$
}
\vskip 3ex
\begin{tcolorbox}[fontupper=\Large,sharp corners, colback=teal!20, colframe=Aquamarine!10!blue]
$ r(s_t,a_t) + \mathbb{E}_{s_{t+1} \sim P(\bullet \mid (s_t,a_t))} \left[\mathbb{V}^{\pi}_{s_{t+1}}\right] $
\end{tcolorbox}
Use the sampled s_{t+1}
\vskip 3ex
\begin{tcolorbox}[fontupper=\Large,sharp corners, colback=teal!20, colframe=Aquamarine!10!blue]
$\mathbb{A}^{\pi}(s_t, a_t) = r(s_t, a_t) + \mathbb{V}^{\pi}(s_{t+1}) - \mathbb{V}^{\pi}(s_t)$
\end{tcolorbox}

\subsection{Estimating $\mathbb{V}^{\pi}$}
\subsubsection{ Version 1 : Monte carlo estimation }

\begin{enumerate}

\item $ \text{Original single sample estimate }\mathbb{V}^{\pi}(s_t) = \sum_{t^{\prime} = t}^T r({s}_{t^{\prime}},a_{t^{\prime}})$
\item $ \text{Multisample estimate (but can't reset the world in practice) }\mathbb{V}^{\pi}(s_t) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \sum_{t^{\prime} = t}^T r({s}_{t^{\prime}},a_{t^{\prime}})$

\end{enumerate}

Step 1 : Aggregate dataset of single sample estimates

\scalebox{1.5}{%
$\left(s_{i,t}, \underbrace{\sum_{t^{\prime} = t}^T r(s_{i,t^{\prime}},a_{i,t^{\prime}})}_{y_{i,t}}\right)$
}


Step 2 :
\scalebox{1.5}{%
$ \mathcal{L}(\phi) = \frac{1}{2} \sum_i \left\Vert \hat{\matchbb{V}}_{\phi}^{\pi}(s_i) - y_i\left\right\Vert_2^2
  {Laplace transform}$
}


\subsubsection{ Version 2 : Bootstrapping}

Ideal target is the true sum of rewards. This is equal to the reward at the current time step and the expected reward at the next time step.

\vskip 3ex
\scalebox{1.5}{%
 $ \text{ Monte carlo target : } y_{i,t} = \sum_{t^{\prime} = t}^T r(s_{i,t^{\prime}},a_{i,t^{\prime}})$
}
\vskip 3ex

\begin{tcolorbox}[fontupper=\Large,sharp corners, colback=gray!20, colframe=green!10!pink]
 $ \text{ Ideal target : }\sum_{t^{\prime} = t}^T \mathbb{E}_{\pi_{\theta}}$
 $r(s_{i,t^{\prime}},a_{i,t^{\prime}}) \mid  (s_{i,t}) \approx r(s_{i,t},a_{i,t})$
 $+ \sum_{t^{\prime} = t + 1}^T \mathbb{E}_{\pi_{\theta}} r(s_{t^{\prime}},a_{t^{\prime}}) \mid (s_{i,t+!})$
$r(s_{i,t},a_{i,t})\mathbb{V}^{\pi}(s_{i,t+1}) \approx r(s_{i,t},a_{i,t})\underbrace{\hat{\mathbb{V}}_{\phi}^{\pi}(s_{i,t+1})}_{\text{Fitted value function}}$
\end{tcolorbox}

\end{document}
