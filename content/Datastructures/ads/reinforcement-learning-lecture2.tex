\documentclass[../../../include/open-logic-chapter]{subfiles}

\begin{document}

\olchapter{ads}{reinforcementlearning}{Lecture IV}

\olsection{Actor Critic methods}
\subsection{Value function}

This is the future rewards starting at s and following policy $\pi$

\scalebox{1.5}{%
$\textstyle V^{\pi}( s_t ) = \sum_{t^{\prime} = t}^T \mathbb{E}_{{s}_{t^{\prime}},a_{t^{\prime}} \sim \pi} \left[r(s_{t^{\prime}},a_{t^{\prime}}) \mid s_t \right] $
}

\subsection{Q-function $\mathbb{Q}^{\pi}$}

\scalebox{1.5}{%
$\textstyle Q^{\pi}( s_t,a_t ) = {\textcolor{blue}{\sum_{t^{\prime} = t}^T }}\mathbb{E}_{\pi} \left[r(s_t,a_t) \mid s_t,a_t \right] $
}

\subsubsection{Improving Policy Gradients}

The gradient would be better if we increased the likelihood when $\mathbb{Q}$ is high and decreased the
likelihood when $\mathbb{Q}$ is low

$\nabla_{\theta} J(\theta) \sim \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} s_{s,i) \mathbb{Q}(s_t^i a_t^i)$

The explanation shown below is about this.

\subsubsection{Introducing baselines}

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim P_{\theta}(\tau)} \left[\nabla_{\theta} \log P{\theta}(\tau) r(\tau) - \textcolor{blue}{\underbrace{b}_{\parbox{3cm}{subtracting  a baseline  ``constant''}}}  \right]$
}

\vskip 3ex
which is the average reward given by
\vskip 3ex
\scalebox{1.5}{%
  $b = \frac{!}{N} \sum_{i=1}^N r(\tau)$
}
\vskip 3ex
If we subtract average reward we get negatie-gradients for below-average behaviour.

The following is the proof that the subtracted value doesn't have any effect on the gradient

$ \mathbb{E}\left[\nabla_{\theta} \log \pi{\theta}(\tau) b\right] = \int p_{\theta}(\tau)\nabla_{\theta} \log \pi{\theta}(\tau) b d\tau $
Using this trick
\begin{tcolorbox}[fontupper=\Large,sharp corners, colback=gray!20, colframe=green!10!pink]

\vskip 3ex
$  P_{\theta}(\tau) \nabla_{\theta} \log  P_{\theta}(\tau) $
$ = \nabla_{\theta}P_{\theta}(\tau)$

\end{tcolorbox}

we get
\scalebox{1.5}{%
$ = b\underbrace{\nabla_{\theta} \underbrace{\int P_{\theta}(\tau) d\tau}_\text{This is 1}}_{\text{Gradient is zero}}$
}
\eject
\begin{figure}[ht]
  \includegraphics{./qfunction.tikz}
  \caption{Improving policy Gradients}
  \ollabel{fig:function}
\end{figure}

This will ameliorate the situation caused when the robot takes a small
step forward and fall backwards.

\subsubsection{Estimating future rewards using $\mathbb{Q}$ function}

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t}) \underbrace{\tcboxmath{r(s_{i,{t^{\prime}}}, a_{i,{t^{\prime}}})}}_{\parbox{3cm}{Extimate future rewards if we take {a_t} \text{ at } {s_t}}}$}



True future rewards can be obtained by using the $\mathbb{Q}$ function like this.

We will get much better results if the form of the gradient is like this.

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t}) \mathbb{Q}(a_{i,t} , s_{i,t}) $
}

\subsubsection{What about baselines ? }

Once we start using the $\mathbb{Q}$ function we get this when we subtract the baseline.

\vskip 3ex

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t}) ( \mathbb{Q}(a_{i,t} , s_{i,t}) - \underbrace{\cancel{b}}_{\mathbb{V}^\pi (s_t)} )$
}

\vskip 3ex

\begin{tcolorbox}[fontupper=\Large,sharp corners, colback=green!10, colframe=green!10!pink]
$ b = \text{ average reward } = \frac{1}{N} \sum_i \mathbb{Q}(s_{i,t}, a_{i,t})$

\vskip 3ex

$\textstyle V( s_t ) = \mathbb{E}_{a_t \sim \pi_{\theta}(\bullet \mid s_t)} \left[\mathbb{Q}(s_t,a_t) \right] $
\end{tcolorbox}

\vskip 3ex
Recall : $\mathbb{A}^{\pi}(a_t, s_t) =  \mathbb{Q}^{\pi}(a_t, s_t)-\mathbb{V}(s_t)$

So with baseline we get

\scalebox{1.5}{%
$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t}) \mathbb{A}^{\pi}(a_t, s_t)$
}

\section{Offline RL outline }

Better estimates of $\mathbb{A}$ will lead to less noisy gradients.
\vskip 3ex
${\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi{\theta} (a_{i,t} \mid s_{i,t})\tcboxmath[
  `colback=teal,
        colframe=green!75!black,
        boxrule=0.5pt,
        arc=2mm
] {\mathbb{A}^{\pi}(a_t, s_t)}}$

First initialize the policy ( randomly with imitation learning and heuristics )

\begin{figure}[ht]
  \includegraphics{./onlinerl.tikz}
  \caption{Online RL}
  \ollabel{fig:function}
\end{figure}
\end{document}
